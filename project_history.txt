Serviva un modo per ottenere le informazioni relative ai paper da analizzare reperibili sul sito della conferenza. 
La metodologia scelta è stata quella dello scraping data l'indispobilità di API pubbliche per l'estrazione dei dati.
In prima battuta si è pensato di optare per il servizio web di jina, 
    il quale dato un https restituisce una versione compatta e schematizzata in markdown del sito. 

Non restituiva però la totalità delle informazioni necessarie (le metareview), 
pertanto ho optato per utilizzare crawl4ai, una libreria dove è possibile delle configurazioni per lo scraping, 
il che lo rende più flessibile e adatto al nostro scopo.
Ho quindi creato un crawler che estrae le informazioni dei paper che si trovano nella 
pagina principale della conferenza (autori, titolo e link al paper), 
    in seguito viene visitata la pagina del singolo paper per estrarre il resto delle 
    informazioni (abstract, link al codice, dataset, DOI, supplementary material, review, author feedback e metareview).

3/11/2025
Sono riuscito a scaricare i dati di tutti i paper dal sito del MICCAI, 
    manca però il testo completo del pdf (da ragionare su come poi salvarlo su SQL), 
    e ci sono alcuni campi che differiscono come nome da quello che poi dovrebbero avere sul db.
    Tali dati li ho salvati in formato JSON, dovrò trovare un modo per poter caricare su db le varie entry.
4/11/2025
Ho provato ad utilizzare loaddata per caricare i dati sul db, 
    ho avuto problemi con la formattazione del mio JSON e con i campi del database già esistente. 
Ho dovuto quindi accedere al db tramite shell per eliminare le tabelle esistenti e rifare le migrations con django.

5/11/2025
Creato lo script load_data.py per caricare sul db i paper. ancora non funziona perchè c'è ancora da controllare cosa fa il codice creato da claude.
Aggiunti modelli del dataset e del PDFPaper
Aggiunto il cleaning degli autori e dei dataset

6/11/2025
Completato il file load_data.py per caricare i dati sul db. Al momento funziona per poter caricare un paper con i relativi datasets

7/11/2025
Aggiunto pdf al model e aggiunto su load_data il carimento del file sul db, iniziato a guardare come estrarre informazioni dal pdf (libreria pyPDF2)

10/11/2025
Aggiunto supplementary material come filefield e caricato sul modello da load_data.py

11/11/2025
Iniziato a testare llm per l'estrazione delle informazioni dal pdf in read_pdf.py

12/11/2025
Riscontrati problemi nell'installare genai con pipenv (non riusciva a completare il locking).
Iniziato a setuppare uv come nuovo gestore al posto di pipenv. Creato dockerfile per uv.

13/11/2025
Integrato in modo completo (spero) uv nel progetto del docker di django-web

14/11/2025
Scancherato con i volumi di docker per ottenere i file creati dal container nell'host, problema non risolto ma forse non è un problema

17/11/2025
Capire se pydantic può essere utile come libreria per ottenere dati formattati da LLM

19/11/2025
Ricerca su API di KIMI k2 (moonshot/byteplus 500k free token)
sistemati volumi docker e messo permessi di root all'utente del container di django altrimenti non andava crawl4ai
Parallelizzato lo scraping dei singoli paper

20/11/2025
Creato un primo prototipo di chiamata a KIMI2, provato a testare il modello sul criterio di annotation. Aggiornato odckerfile

21/11/2025
Primi test funzionanti con kimi2, stesso risultato tra il testo in cui ho fornito il paper in formato txt e il testo in formato pdf
Iniziata creazione della struttura dei criteri in modo che un LLM possa capire cosa cercare nel testo
Testati modelli kimi2 e deepseekv3 su struttura di base, danno risposte diverse

24/11/2025
Aggiunti modelli seed, deepseek v3.1 con e senza thinking ai test. Separato il task di trovare il testo legato ad un criterio da quello di valutare tale testo.
Dall'unico test effettuato, i migliori risultati si ottengono da deepseek v3.1 e kimi. 
Il COT di deepseek aiuta ma la lentezza nel fornire la risposta potrebbe essere un problema siccoma è 10 volte più lento. 
Va valutata la qualità dell'informazione in più ottenuta

25/11/2025
Divisa la parte di estrazione del testo da quella di scoring in due file distinti.
La divisione non è stata una buona idea dato il raddoppio delle tempistiche nell'ottenere il risultato (20 sec per deepseek senza COT che era il più veloce).
Aggiunto anche gpt e gemini ma non testato.

Bloccato su come tenere traccia dei token utilizzati per ogni LLM.

26/11/2025
Creato file per tenere traccia dei token utilizzati per ogni LLM.
gpt5.1 - Input tokens: 8483, Output tokens: 1403, Cumulative tokens: 9886, Time: 17.29s
gpt5_mini - Input tokens: 8483, Output tokens: 3097, Cumulative tokens: 11580, Time: 39.24s
gemini2.5_flash - Input tokens: 9207, Output tokens: 933, Cumulative tokens: 13783, Time: 21.21s


01/12/2025
Aggiunta pagina di profilo utente con storico analisi
Aggiornato API GPT con responses al posto di chat completions (utilizzando json_schema)
Aggiunto logout

02/12/2025
Iniziato ad approcciare i tool di gpt per ottenere la documentazione del codice
Aggunto feedback/bug report sul sito 
Aggiornato stile sito

03/12/2025
Aggiunta documentazione codice alla valutazione di gpt5.1
Completata implementazione del tool per ottenere il codice
Aggiunta funzionalità di controllo limite token giornaliero

04/12/2025
Aggiunto tasto per selezionare tutti i modelli nella pagina di analisi
Codice non viene utilizzato nel modello, 

05/12/2025
Sistemato prompt per ottenere valutazione più accurata.

09/12/2025
Rimossi modelli cinesi
Rimosso tool di gpt per il codice (non veniva chiamato, probabilmente per il prompt)
abilitata variabile PYTHONUNBUFFERED per vedere le print sul terminale

10/12/2025
Aggiunto controllo paper già analizzati mostrando la history (query con la prima linea del testo, parte del titolo)
Sistemato retrieve del codice: Aggiunta regex per trovare codice nel testo del paper. Se il codice è di github
minor graphic update

11/12/2025
Aggiunto peso ai criteri di valutazione
Creato modello per prompt

12/12/2025
Mergiata visuale dettagli delle analisi

15/12/2025
Eseguita separazione produzione e sviluppo (separazione container)
switch feedback from "button" to "a"

16/12/2025
Sistemato problema nella separazione produzione/sviluppo. Creati due progetti docker

17/12/2025
Aggiunta fixture automatica
sostituita lo storing in-memory con un modello AnalysisTask per tenere traccia delle task per le analisi

18/12/2025 
Creato excel per gestione prompt
Guardato tool per estrarre titolo dal pdf (GROBID e science-parse)
Guardato modelli per classificazione paper
Iniziato a provare ad uploadare i pdf a gpt

22/12/2025
implementato grobid per estrazione testo + titolo dal pdf

23/12/2025
Finita implementazione di grobid
Fornire PDF a GPT non permette di ottenere il testo estratto. Come capire se basa il suo score in modo giusto?

12/01/2026
Iniziato a guardare celery per task asincrone per la gestione delle analisi e il web crawling (crawl itself e per la queue di analisi di molti paper trovati duranti il crawling)

13/01/2026
Implementata struttura base celery + redis per task asincrone (only dev)
Aggiunto spinner nei log di analisi

14/01/2026
Splittate le chiamate all'LLM per ogni criterio. Non vengono però visualizzate correttamente, controllare come JS gestisce la risposta

15/01/2026
Corretta la visualizzazione dei risultati delle analisi dopo lo split per criterio
Effettuata modifca per le chiamate in parallelo. Manca la parallelizzazione in caso multipli modelli vengono chiamati.
Creato docker per celery, redis e grobid in production

19/01/2026
Aggiornato prompt seguendo consigli di LLM-as-a-judge and GEval

21/12/2026
Iniziato a valutare di utilizzare kimi k2. Con turbo l'output è veramente veloce (60-100 token al secondo), costa meno di GPT (8$ vs 10$ 1M token in output), è possibile generare i logprobs.
Occorre testare sia il preview che il turbo e confrontarli con GPT5.1/GPT5.2
Occorre testare anche il reasoning di KIMI k2

22/01/2026
Manca solo implementazione su server di KIMI K2. Niente reasoning perchè non ha i logprobs. Le valutazioni sembrano simili a GPT

23/01/2026
Terminare compilazione grafo e capire come gestire evidence locator

26/01/2026
Iniziata implementazione linter per l'analisi statica del codice

27/01/2026
Definito gli evidence locator

28/\01/2026
Definito in modo più dettagliato la valutazione degli evidence locator (da finire)


TODO:
Short term:
- Ripensare evidence locator (Unico che comprende tutti i tipi di paper, oppure farne N+1 - Un generico e uno per ogni tipo di paper)
- Pensare al prompt per l'evidence locator
- Ragionare su come gestire l'estrazione di parti salienti del testo e la valutazione tramite evidence locator
- Finire di definire i campi e i valori attesi riguardo gli experimental resualts (Utilizzare come riferimento la chat di gemini "Reviewing AI model assumptions")

- Chiedere CPU per far girare GROBID e vedere se si velocizza 
- Analisi statica del codice (per vedere se non ci sono errori di sintassi trovabili senza LLM, esistono tool per fare ciò) - troppi errori inutili trovati da ruff e company, aggiustare da impostazioni (https://docs.astral.sh/ruff/linter/#rule-selection)
- combinazione risultato multimodello
- Trovare un modo per analizzare il codice (max 400k context)
- Notifica quando l'analisi è conclusa

- Spostare fuori da llm_analysis (in run_celery_analysis_tasks) la creazione dell'user prompt in modo che sia differente sulla base del criterio (es. codice solo per code documentation)
   
    - Fare in modo che il risultato dell'analisi sia una richiesta get di AnalysisDetailView
    CODICE
    - ottenere il codice da fare valutare
    - Parallelizzare valutazione codice con quella del paper (per ridurre tempistiche)
    

    - Ricerca: Confrontrare paper caricato con quelli nel DB (titolo ok anche per scraping)
    - Valutazione: Ottenere testo da valutare senza LLM
    PAPER ELABORATION
    - Classificazione del tipo di paper (survey, method, dataset, benchmark, ) per adattare i criteri di valutazione
    - Gestione dei criteri da fornire all'LLM (variabili in base al tipo di paper)
    - Guardare GROBID e Science-parse per ottenere titolo
    - Cercare modello di classificazione di paper (survey, method, dataset, benchmark) Qwen/Qwen2.5-0.5B-Instruct oppure meta-llama/Llama-3.2-1B-Instruct


Long term:
    - Valutare la resistenza all'injection resistance della pipeline di valutazione
    - Integrare celery + redis per crawling in background
    - Aggiungere badge per indicare nell'icona del profilo una nuova analisi completata non letta
    - Rendere i risultati di ricerca dell'analisi collassabili
    - Messaggi di risposta a feedback utenti
    - Gestione feedback avanzata (schermata appropriata, avanzamento richiesta, sollecitamento, etc)
    - Integrazione mail (notifiche analisi, feedback, etc)
    - Modifica button New analysis (history API)

    - Visualizzazione migliore dei feedback
    - Gestione permessi utente
    - Mergare output di diversi LLM
    - Chiedere mail al signup (opzionale)
    - Permettere all'utente di modificare le proprie info (e API)
    
    - Controllare cosa è fattibile valutare senza LLM e con LLM
    - Parallelizzare la lettura dei pdf
    - Valutare i modelli lanciando le chiamate su 5 paper
    
    - creare gruppi di utenti con diversi limiti di token (check_token_limit in functions.py)
    


DOMANDE:
    - Come controllare se il pdf che c'è online sia aggiornato? è possibile che venga cambiato nel tempo?
    - Come ottenere i link dei dataset dal pdf del paper, si potrebbe avere a disposizione solamente la citazione
    - Documenti creati dal container non disponibili nell'host ( a meno che il volume non sia vuoto), ci interessa avere?
    - Come gestire il codice (ottenibile da gitingest, https://gitingest.com/)
    - Qual è la palette di colore da associare ad un progetto del genere? (Blu profondo (#0A3D62 – #1B4F72) - Grigio neutro e grafite (#5A5A5A – #2E2E2E) - Ciano/Teal tecnologico (#1ABC9C – #0E8C82) - Ambra o giallo controllato (#F1C40F – #E2B90A) per alert - Bianco sporco (#F7F7F7) per sfondo al posto del bianco profondo)
    - Ha senso dividere il task di estrazione delle informazioni da quello di scoring?
    
SCORE:
- Quali parti dello scoring posso essere affidate ad un LLM?
- Quali parti dello scoring posso fare con regole fisse?
- Quali parti dello scoring richiedono l'analisi del pdf?
- Ha senso introdurre penalizzazioni in fase di scoring?
- Creare una tabella del DB con le info retrivate relative ad ogni metrica di valutazione, lo score assegnato e il relativo peso.
    In questo modo si sa su che cosa un eventale LLM ha basato il suo giudizio


-- Data: Questo criterio valuta la presenza, la disponibilità dei dati utilizzati dati utilizzati dovrebbero essere descritti all'interno del paper, 
    nella pagina del paper sono solo visibili i link ai dataset. 
    Forse è plausibile assumere che i dataset siano pubblici se è presente il link. 
    Per il resto dobbiamo affidarci alla descrizione nel paper per la valutazione
-- Code: il codice utilizzato dovrebbe essere presente all'interno dell'abstract,
    ma non so se è una regola ferrea ppure no, nel caso del MICCAI è anche disponibile sul sito.
    Si potrebbe pensare ad un primo check sul sito, se non è presente passare al pdf. 
    Se non è presente nemmeno lì possiamo assumere che non sia disponibile.
    Il codice però non è strettamente necessario per il funzionamento del paper, 
    come anche indicato dalla conferenza. 
    Pertanto il codice dovrà rientrare nello scoring solamente se fa parte della riproducibilità.
    La presenza del solo link non è sufficiente per una buona valutazione del codice siccome la repo potrebbee 
    essere vuota, incompleta o non funzionante.
    C'è quindi da analizzare quanto meno il readme,la documentazione, o esempi di funzionamento (colab, container) 
    resa disponibile nella repo
-- Annotation: questo criterio valuta le informazioni relative a come sono state eseguite le annotazioni, 
    il grado di esperienza dei valutatori, e che metriche sono state utilizzate quanto i valutatori sono d'accordo tra loro
    Tali informazioni si trovano solo all'interno del paper,  LLM per estrazione e lunghezza per valutazione no LLM.
    Andrebbe anche considerato se c'è un senso logico, solo tramite LLM
-- Preprocessing: La valutazione si basa su quanto è accurata e documentata. 
    Queste informazioni che si trova all'interno del paper. LLM per estrazione e lunghezza per valutazione no LLM
    Andrebbe anche considerato se c'è un senso logico, solo tramite LLM
-- Evaluation: Informazioni presenti all'interno del paper.
    Valuta la completezza e la chiarezza della sezione dedicata alla valutazione sperimentale. LLM per estrazione e lunghezza per valutazione no LLM
    Andrebbe anche considerato se c'è un senso logico, solo tramite LLM
-- Documentazione: raggruppabile nella valutazione del codice
-- Licensing and ethical transparency, raggruppabile nei data
    



Here’s a direct, clean English translation:

-- Data: This criterion assesses the presence and availability of the data used. The data should be described within the paper; on the paper’s webpage, only the dataset links are visible. It may be reasonable to assume that datasets are public if a link is provided. Beyond that, we must rely on the description in the paper for the evaluation.

-- Code: The code used should be included in the abstract, though I’m not sure this is a strict rule. In the case of MICCAI, it’s also available on the website. One could first check the conference site; if it’s not there, move to the PDF. If it’s not there either, we can assume it’s unavailable.
The code, however, isn’t strictly required for the paper to function, as the conference itself indicates. Therefore, code should contribute to the scoring only when it is part of reproducibility.
A link alone isn’t enough for a solid code evaluation, since the repository may be empty, incomplete, or non-functional. So the README, documentation, or available usage examples (Colab, container) in the repo need to be examined at minimum.

-- Annotation: This criterion evaluates information on how annotations were performed, the expertise level of the annotators, which metrics were used, and how much the annotators agreed with one another. These details are found only within the paper. LLMs can be used for extraction; length-based scoring should avoid LLMs. Logical consistency should also be considered, using an LLM only for that part.

-- Preprocessing: The evaluation depends on how accurate and well-documented the preprocessing is. These details are found within the paper. LLMs for extraction, and length-based scoring without LLMs. Logical consistency should also be considered, using an LLM only for that.

-- Evaluation: Information found within the paper. This assesses the completeness and clarity of the experimental evaluation section. LLMs for extraction; length-based scoring without LLMs. Logical consistency should also be considered, using an LLM only for that.

-- Documentation: Can be grouped under code evaluation.

-- Licensing and ethical transparency: Can be grouped under data.


Paper caricato -> controllo se esiste già (titolo) -> controllo se c'è già il codice (code_text) -> [si] Mostro analisi -> chiedo nuova analisi -> abilito modelli -> chiedo conferma per sovrascrittura analisi 
                                                                                                 -> [No] Abilito code_tool x gpt -> Abilito modelli


CELERY:
• Asynchronous Task Execution
• Distributed Task Queue
• Task Scheduling and Periodic Tasks
• Result Handling
• Error Handling and Retry Mechanism
• Monitoring and Management




reproducibility_mapping = {
    # -------------------------------------------------------------------------
    # Models and Algorithms
    # -------------------------------------------------------------------------
    "mathematical_setting": {
        "objective_function": True/False, 
        "loss_formulation": True/False, 
        "optimization_constraints": True/False, 
        "variable_definitions": True/False,
    },
    "algorithm_description": {
        "pseudocode": True/False, 
        "architecture_diagrams": ???, #come poter far capire all'llm se c'è o meno? 
        "update_rules": True/False, 
        "convergence_criteria": True/False
    },
    "model_assumptions": {
        "linearity_constraints": [], 
        "distributional_priors": [], 
        "stationarity_assumptions": [], 
        "independence_assumptions": []
    },
    "software_frameworks": {
        "libraries_used": True/False, 
        "version_numbers": True/False, 
        "cuda_or_backend_versions": True/False, 
        "random_state_seeding": True/False
    },

    # -------------------------------------------------------------------------
    # Datasets
    # -------------------------------------------------------------------------
    "dataset_statistics": {
        "sample_counts": True/False, 
        "class_balance_distribution": True/False, 
        "missing_data_rates": True/False, 
        "feature_dimensionality": True/False,
        "feature_correlation_analysis": True/False,
        "data_leakage": True/False
    },
    "study_cohort_description": {
        "demographics": True/False, 
        "inclusion_criteria": True/False, 
        "exclusion_criteria": True/False, 
        "population_size": True/False
    },
    "dataset_metadata": { 
        "doi": True/False,
        "source_url": True/False, 
        "license_type": True/False, 
        "versioning": True/False
    },
    "data_collection_process": {
        "data_source": True/False,
        "sampling_methodology": True/False, 
        "temporal_period": True/False,
        "survey_design": True/False, 
        "cleaning_preprocessing": True/False, 
        "expert_review_process": True/False
    },
    "acquisition_setup": {
        "device_specifications": True/False, 
        "environmental_conditions": True/False, 
        "calibration_procedures": True/False, 
        "acquisition_parameters": True/False # generalization of sampling_rate, exposure, etc.
    },
    "annotation_process": {
        "labeling_guidelines": True/False, 
        "annotator_qualifications": True/False, 
        "consensus_protocols": True/False, 
        "inter_rater_reliability": True/False
    },
    "quality_control": {
        "outlier_detection_methods": True/False, 
        "manual_audits": True/False, 
        "validation_checks": True/False, 
        "automated_filtering": True/False
    },
    "availability_and_ethics": {
        "repository_links": True/False, 
        "access_permissions": True/False, 
        "ethics_approval_id": True/False, 
        "anonymization_protocol": True/False
    },

    # -------------------------------------------------------------------------
    # Code
    # -------------------------------------------------------------------------
    "environment_setup": {
        "dependency_files": [], # e.g., requirements.txt, conda.yaml
        "container_definitions": [], # e.g., Dockerfile
        "setup_scripts": [], 
        "os_requirements": []
    },
    "implementation_scripts": {
        "training_logic": [], 
        "inference_logic": [], 
        "model_definitions": [], 
        "utility_functions": []
    },
    "reproducibility_artifacts": {
        "pretrained_weights": [], 
        "checkpoints": [], 
        "configuration_files": [], 
        "logging_outputs": []
    },
    "documentation": {
        "usage_instructions": [], 
        "execution_commands": [], 
        "readme_content": [], 
        "argument_definitions": []
    },

    # -------------------------------------------------------------------------
    # Experimental Results
    # -------------------------------------------------------------------------
    "hyperparameters": {
        "optimization_parameters": [], # e.g., learning rate, momentum
        "batch_sizes": [], 
        "search_strategy": [], # e.g., grid search, optuna
        "architectural_hyperparams": []
    },
    "sensitivity_analysis": {
        "ablation_studies": [], 
        "perturbation_tests": [], 
        "robustness_metrics": [], 
        "variance_analysis": []
    },
    "training_dynamics": {
        "epoch_counts": [], 
        "stopping_criteria": [], 
        "random_seeds_used": [], 
        "iterations": []
    },
    "comparative_analysis": {
        "baseline_comparisons": [], 
        "sota_benchmarks": [], 
        "implementation_variants": [], 
        "default_settings_comparison": []
    },
    "evaluation_setup": {
        "data_split_ratios": [], 
        "stratification_method": [], 
        "cross_validation_k": [], 
        "metric_definitions": [] # Replaces specific metrics like f1_score
    },
    "statistical_results": {
        "central_tendency_measures": [], # mean, median
        "dispersion_measures": [], # std dev, error bars
        "significance_tests": [], # t-test, ANOVA
        "confidence_intervals": []
    },
    "resource_consumption": {
        "execution_time": [], 
        "latency_metrics": [], 
        "energy_footprint": [], 
        "memory_requirements": []
    },
    "infrastructure_used": {
        "hardware_specifications": [], # GPU/CPU models
        "compute_environment": [], # Cloud vs Local
        "parallelization_setup": [], 
        "storage_requirements": []
    },
    "qualitative_analysis": {
        "failure_modes": [], 
        "limitation_discussion": [], 
        "clinical_relevance": [], 
        "real_world_impact": []
    }
}

evaluation_metrics -> lista di metriche
statistical_significance_analysis -> lista di statistiche utilizzate
failure_analysis -> dove fallisce il modello/soluzione
pretrained_models -> weights/checkpoints
run_commands_and_readme -> documentaiozione

reproducibility_framework = {
    # Models and Algorithms
    "models_and_algorithms": {
        "mathematical_setting": {
            "objective_function": True/False, 
            "loss_formulation": True/False, 
            "optimization_constraints": True/False, 
            "variable_definitions": True/False,
            "model_assumptions": ["Explicit", "Implicit", "Missing"], # è possibile valutarle in modo più dettagliato? Spesso non sono implicite nell'architettura che viene presentata?
        },
        "algorithm_description": {
            "pseudocode": True/False, *
            "architecture_diagrams": ???, #come poter far capire all'llm se c'è o meno? 
            "update_rules": True/False, 
            "convergence_criteria": True/False
        },
    },
    
    # Datasets
    "datasets": {
        "dataset_statistics": {
            "sample_counts": True/False, 
            "class_balance_distribution": ["Balanced", "Slight Imbalance", "Severe Imbalance", "Not Reported"], 
            "missing_data_rates": True/False, 
            "feature_dimensionality": True/False,
            "data_leakage": ["Checked & Mitigated", "Potential Risk", "Not Discussed"]
        },
        "study_cohort_description": {
            "demographics": ["Full", "Partial", "Missing"], 
            "inclusion_criteria": True/False, 
            "exclusion_criteria": True/False, 
            "population_size": True/False
        },
        "dataset_metadata": {
            "citations_and_doi": True/False, 
            "source_url": ["Public", "Request-only", "Private", "Broken Link"], 
            "license_type": ["Open", "Academic-Only", "Proprietary", "Unknown"], # Può bastare un True/False?
            "versioning": True/False
        },
        "data_collection_process": {
            "data_source": True/False,
            "sampling_methodology": True/False, 
            "temporal_period": True/False,
            "survey_design": True/False, 
            "cleaning_preprocessing": True/False, 
            "expert_review_process": True/False
        },
        "acquisition_setup": {
            "device_specifications": ["Multiple Vendors", "Multiple Devices", "Single Device", "Unknown"], 
            "environmental_conditions": True/False, 
            "calibration_procedures": True/False, 
            "acquisition_parameters": True/False # generalization of sampling_rate, exposure, etc.
        },
        "subjects_objects_involved": [],
        "annotation_instructions": {
            "labeling_guidelines": True/False, 
            "annotator_expertise": ["Expert", "Trained", "Crowd", "Algorithm"],
            "consensus_protocols": True/False, 
            "inter_rater_reliability_score": ["Strong", "Weak", "Not Reported"]
        },
        "quality_control": {
            "outlier_detection_methods": True/False, 
            "manual_audits": True/False, ??
            "validation_checks": True/False, ??
            "automated_filtering": True/False, ??
            "bias_analysis": True/False
        },
        "availability_and_ethics": {
            "repository_links": True/False, 
            "access_permissions": True/False, 
            "ethics_approval_id": ["Provided", "Generic statement", "Missing", "Not Applicable"], 
            "anonymization_protocol": True/False
        },
    },
    
    # Code
    "code_artifacts": {
        "environment_setup": {
            "libraries_used": ["Full", "Partial", "Missing"],
            "version_numbers": ["Full", "Partial", "Missing"],
            "container_definitions": ["Full", "Partial", "Missing", "Not Applicable"],
            "os_requirements": ["Full", "Partial", "Missing", "Not Applicable"],
            "cuda_or_backend_info": True/False,
            "random_state_seeding": ["Full", "Partial", "Missing", "Not Applicable"]
        },
        "implementation_scripts": {
            "script_setup": True/False,
            "training_logic": True/False, 
            "inference_logic": True/False, 
            "preprocessing_details": ["Full", "Partial", "Missing", "Not Applicable"]
        },
         "reproducibility_artifacts": {
            "checkpoints": True/False, 
            "configuration_files": ["Full", "Partial", "Missing", "Not Applicable"], 
            "logging_outputs": ["Full", "Partial", "Missing", "Not Applicable"],
            "documentation": ["Full", "Partial", "Missing"]
        },
        "code_availability": {
            "repository_link_status": ["Active/Public", "Broken", "Request Access", "None"],
            "training_script": True/False,
            "evaluation_script": True/False,
            "checkpoints": True/False,
            "preprocessing_details": ["Full", "Partial", "Missing", "Not Applicable"],
            "logging_outputs": ["Full", "Partial", "Missing", "Not Applicable"],
            "readme_quality": ["Full", "Partial", "None"] 
        }
    },
    
    # Experimental Results
    "experimental_results": {
        "hyperparameters": {
            "optimization_parameters": ,
            "batch_sizes": True/False, 
            "search_strategy": True/False,
            "architectural_hyperparams": [*hyperparameters list*]
        },
        "sensitivity_analysis": {
            "ablation_studies": True/False, 
            "perturbation_tests": True/False, 
            "evaluation_metrics": [*metrics list*],
            "statistical_analysis": [*statistics list*],
    },
        "training_and_evaluation_runs_count": [],
        "baseline_implementation_details": ["Full", "Partial", "Missing", "Not Applicable"],
        "data_splits_definition": True/False,
        
        
        "central_tendency_and_variation": True/False,
        "runtime_and_energy_cost": True/False,
        "memory_footprint": True/False,
        "failure_analysis": True/False,
        "computing_infrastructure": True/False,
        "clinical_significance_discussion": True/false
    },

    "missing_candidates": []

}
