Serviva un modo per ottenere le informazioni relative ai paper da analizzare reperibili sul sito della conferenza. 
La metodologia scelta è stata quella dello scraping data l'indispobilità di API pubbliche per l'estrazione dei dati.
In prima battuta si è pensato di optare per il servizio web di jina, 
    il quale dato un https restituisce una versione compatta e schematizzata in markdown del sito. 

Non restituiva però la totalità delle informazioni necessarie (le metareview), 
pertanto ho optato per utilizzare crawl4ai, una libreria dove è possibile delle configurazioni per lo scraping, 
il che lo rende più flessibile e adatto al nostro scopo.
Ho quindi creato un crawler che estrae le informazioni dei paper che si trovano nella 
pagina principale della conferenza (autori, titolo e link al paper), 
    in seguito viene visitata la pagina del singolo paper per estrarre il resto delle 
    informazioni (abstract, link al codice, dataset, DOI, supplementary material, review, author feedback e metareview).

3/11/2025
Sono riuscito a scaricare i dati di tutti i paper dal sito del MICCAI, 
    manca però il testo completo del pdf (da ragionare su come poi salvarlo su SQL), 
    e ci sono alcuni campi che differiscono come nome da quello che poi dovrebbero avere sul db.
    Tali dati li ho salvati in formato JSON, dovrò trovare un modo per poter caricare su db le varie entry.
4/11/2025
Ho provato ad utilizzare loaddata per caricare i dati sul db, 
    ho avuto problemi con la formattazione del mio JSON e con i campi del database già esistente. 
Ho dovuto quindi accedere al db tramite shell per eliminare le tabelle esistenti e rifare le migrations con django.

5/11/2025
Creato lo script load_data.py per caricare sul db i paper. ancora non funziona perchè c'è ancora da controllare cosa fa il codice creato da claude.
Aggiunti modelli del dataset e del PDFPaper
Aggiunto il cleaning degli autori e dei dataset

6/11/2025
Completato il file load_data.py per caricare i dati sul db. Al momento funziona per poter caricare un paper con i relativi datasets

7/11/2025
Aggiunto pdf al model e aggiunto su load_data il carimento del file sul db, iniziato a guardare come estrarre informazioni dal pdf (libreria pyPDF2)

10/11/2025
Aggiunto supplementary material come filefield e caricato sul modello da load_data.py

11/11/2025
Iniziato a testare llm per l'estrazione delle informazioni dal pdf in read_pdf.py

12/11/2025
Riscontrati problemi nell'installare genai con pipenv (non riusciva a completare il locking).
Iniziato a setuppare uv come nuovo gestore al posto di pipenv. Creato dockerfile per uv.

13/11/2025
Integrato in modo completo (spero) uv nel progetto del docker di django-web

14/11/2025
Scancherato con i volumi di docker per ottenere i file creati dal container nell'host, problema non risolto ma forse non è un problema

17/11/2025
Capire se pydantic può essere utile come libreria per ottenere dati formattati da LLM

19/11/2025
Ricerca su API di KIMI k2 (moonshot/byteplus 500k free token)
sistemati volumi docker e messo permessi di root all'utente del container di django altrimenti non andava crawl4ai
Parallelizzato lo scraping dei singoli paper

20/11/2025
Creato un primo prototipo di chiamata a KIMI2, provato a testare il modello sul criterio di annotation. Aggiornato odckerfile

21/11/2025
Primi test funzionanti con kimi2, stesso risultato tra il testo in cui ho fornito il paper in formato txt e il testo in formato pdf
Iniziata creazione della struttura dei criteri in modo che un LLM possa capire cosa cercare nel testo
Testati modelli kimi2 e deepseekv3 su struttura di base, danno risposte diverse

24/11/2025
Aggiunti modelli seed, deepseek v3.1 con e senza thinking ai test. Separato il task di trovare il testo legato ad un criterio da quello di valutare tale testo.
Dall'unico test effettuato, i migliori risultati si ottengono da deepseek v3.1 e kimi. 
Il COT di deepseek aiuta ma la lentezza nel fornire la risposta potrebbe essere un problema siccoma è 10 volte più lento. 
Va valutata la qualità dell'informazione in più ottenuta

25/11/2025
Divisa la parte di estrazione del testo da quella di scoring in due file distinti.
La divisione non è stata una buona idea dato il raddoppio delle tempistiche nell'ottenere il risultato (20 sec per deepseek senza COT che era il più veloce).
Aggiunto anche gpt e gemini ma non testato.

Bloccato su come tenere traccia dei token utilizzati per ogni LLM.

26/11/2025
Creato file per tenere traccia dei token utilizzati per ogni LLM.
gpt5.1 - Input tokens: 8483, Output tokens: 1403, Cumulative tokens: 9886, Time: 17.29s
gpt5_mini - Input tokens: 8483, Output tokens: 3097, Cumulative tokens: 11580, Time: 39.24s
gemini2.5_flash - Input tokens: 9207, Output tokens: 933, Cumulative tokens: 13783, Time: 21.21s


01/12/2025
Aggiunta pagina di profilo utente con storico analisi
Aggiornato API GPT con responses al posto di chat completions (utilizzando json_schema)
Aggiunto logout

02/12/2025
Iniziato ad approcciare i tool di gpt per ottenere la documentazione del codice
Aggunto feedback/bug report sul sito 
Aggiornato stile sito

03/12/2025
Aggiunta documentazione codice alla valutazione di gpt5.1
Completata implementazione del tool per ottenere il codice
Aggiunta funzionalità di controllo limite token giornaliero

04/12/2025
Aggiunto tasto per selezionare tutti i modelli nella pagina di analisi
Codice non viene utilizzato nel modello, 

TODO:
    - Mergare output di diversi LLM
    - Controllare cosa è fattibile valutare senza LLM e con LLM
    - Parallelizzare la lettura dei pdf
    - Creare la pipeline che prenda i paper dal db, con anche le review, l'author feedback e le metareview. Combinare il tutto per poi passarlo agli LLM.
    - GPT sembra il migliore nel rispondere, valutare la risposta per vedere se è così. Valutare anche che differenza c'è con la versione mini.
    - Valutare i modelli lanciando le chiamate su 5 paper
    - Dall'output degli LLM prendere il codice e confrontarlo con quello che c'è nel db, se presente
    - Ottenere ciò che interessa dal codice (documentazione, markdown, etc.)

    - Chiedere mail al signup (opzionale)
    - Permettere all'utente di modificare le proprie info (e API)
    
    - Aggiungere combinazione score con pesi (mostrarlo nell'history e aggiungerlo a modello)

    - refinire conteggio token che si resetta ogni giorno
    - Controllare se il paper caricato sia già stato analizzato (confronto titolo o hash),se lo è mostrare le analisi presenti. Chiedere conferma all'utente per continuare
    - Tasto per selezionare tutti i modelli da utilizzare
    - Valutazione non applicabile (lato LLM devono fornire voto negativo)
    - correggere prompt per migliorare risultati
    - Eseguire media per ottenere score finale di un modello, e successivamente calcolare score inter model (calcolando la media di tutte le medie dei modelli, posso tenere una sola analisi per modello di un paper ?)
    
    - Controllare pipline di ingestione codice e verificare che funziona in tutti i casi


DOMANDE:
    - Come controllare se il pdf che c'è online sia aggiornato? è possibile che venga cambiato nel tempo?
    - Come ottenere i link dei dataset dal pdf del paper, si potrebbe avere a disposizione solamente la citazione
    - Documenti creati dal container non disponibili nell'host ( a meno che il volume non sia vuoto), ci interessa avere?
    - Come gestire il codice (ottenibile da gitingest, https://gitingest.com/)
    - Qual è la palette di colore da associare ad un progetto del genere? (Blu profondo (#0A3D62 – #1B4F72) - Grigio neutro e grafite (#5A5A5A – #2E2E2E) - Ciano/Teal tecnologico (#1ABC9C – #0E8C82) - Ambra o giallo controllato (#F1C40F – #E2B90A) per alert - Bianco sporco (#F7F7F7) per sfondo al posto del bianco profondo)
    - Ha senso dividere il task di estrazione delle informazioni da quello di scoring?
    
SCORE:
- Quali parti dello scoring posso essere affidate ad un LLM?
- Quali parti dello scoring posso fare con regole fisse?
- Quali parti dello scoring richiedono l'analisi del pdf?
- Ha senso introdurre penalizzazioni in fase di scoring?
- Creare una tabella del DB con le info retrivate relative ad ogni metrica di valutazione, lo score assegnato e il relativo peso.
    In questo modo si sa su che cosa un eventale LLM ha basato il suo giudizio


-- Data: Questo criterio valuta la presenza, la disponibilità dei dati utilizzati dati utilizzati dovrebbero essere descritti all'interno del paper, 
    nella pagina del paper sono solo visibili i link ai dataset. 
    Forse è plausibile assumere che i dataset siano pubblici se è presente il link. 
    Per il resto dobbiamo affidarci alla descrizione nel paper per la valutazione
-- Code: il codice utilizzato dovrebbe essere presente all'interno dell'abstract,
    ma non so se è una regola ferrea ppure no, nel caso del MICCAI è anche disponibile sul sito.
    Si potrebbe pensare ad un primo check sul sito, se non è presente passare al pdf. 
    Se non è presente nemmeno lì possiamo assumere che non sia disponibile.
    Il codice però non è strettamente necessario per il funzionamento del paper, 
    come anche indicato dalla conferenza. 
    Pertanto il codice dovrà rientrare nello scoring solamente se fa parte della riproducibilità.
    La presenza del solo link non è sufficiente per una buona valutazione del codice siccome la repo potrebbee 
    essere vuota, incompleta o non funzionante.
    C'è quindi da analizzare quanto meno il readme,la documentazione, o esempi di funzionamento (colab, container) 
    resa disponibile nella repo
-- Annotation: questo criterio valuta le informazioni relative a come sono state eseguite le annotazioni, 
    il grado di esperienza dei valutatori, e che metriche sono state utilizzate quanto i valutatori sono d'accordo tra loro
    Tali informazioni si trovano solo all'interno del paper,  LLM per estrazione e lunghezza per valutazione no LLM.
    Andrebbe anche considerato se c'è un senso logico, solo tramite LLM
-- Preprocessing: La valutazione si basa su quanto è accurata e documentata. 
    Queste informazioni che si trova all'interno del paper. LLM per estrazione e lunghezza per valutazione no LLM
    Andrebbe anche considerato se c'è un senso logico, solo tramite LLM
-- Evaluation: Informazioni presenti all'interno del paper.
    Valuta la completezza e la chiarezza della sezione dedicata alla valutazione sperimentale. LLM per estrazione e lunghezza per valutazione no LLM
    Andrebbe anche considerato se c'è un senso logico, solo tramite LLM
-- Documentazione: raggruppabile nella valutazione del codice
-- Licensing and ethical transparency, raggruppabile nei data
    



Here’s a direct, clean English translation:

-- Data: This criterion assesses the presence and availability of the data used. The data should be described within the paper; on the paper’s webpage, only the dataset links are visible. It may be reasonable to assume that datasets are public if a link is provided. Beyond that, we must rely on the description in the paper for the evaluation.

-- Code: The code used should be included in the abstract, though I’m not sure this is a strict rule. In the case of MICCAI, it’s also available on the website. One could first check the conference site; if it’s not there, move to the PDF. If it’s not there either, we can assume it’s unavailable.
The code, however, isn’t strictly required for the paper to function, as the conference itself indicates. Therefore, code should contribute to the scoring only when it is part of reproducibility.
A link alone isn’t enough for a solid code evaluation, since the repository may be empty, incomplete, or non-functional. So the README, documentation, or available usage examples (Colab, container) in the repo need to be examined at minimum.

-- Annotation: This criterion evaluates information on how annotations were performed, the expertise level of the annotators, which metrics were used, and how much the annotators agreed with one another. These details are found only within the paper. LLMs can be used for extraction; length-based scoring should avoid LLMs. Logical consistency should also be considered, using an LLM only for that part.

-- Preprocessing: The evaluation depends on how accurate and well-documented the preprocessing is. These details are found within the paper. LLMs for extraction, and length-based scoring without LLMs. Logical consistency should also be considered, using an LLM only for that.

-- Evaluation: Information found within the paper. This assesses the completeness and clarity of the experimental evaluation section. LLMs for extraction; length-based scoring without LLMs. Logical consistency should also be considered, using an LLM only for that.

-- Documentation: Can be grouped under code evaluation.

-- Licensing and ethical transparency: Can be grouped under data.


Paper caricato -> controllo se esiste già (titolo) -> controllo se c'è già il codice (code_text) -> [si] Mostro analisi -> chiedo nuova analisi -> abilito modelli -> chiedo conferma per sovrascrittura analisi 
                                                                                                 -> [No] Abilito code_tool x gpt -> Abilito modelli

