KIMI K2 
It is a Mixture-of-Experts (MoE) foundation model with exceptional coding and agent capabilities, featuring 1 trillion total parameters and 32 billion activated parameters. In benchmark evaluations covering general knowledge reasoning, programming, mathematics, and agent-related tasks, the K2 model outperforms other leading open-source models
- kimi-k2-0711-preview: Context length 128k
- kimi-k2-0905-preview: Context length 256k. Based on kimi-k2-0711-preview, with enhanced agentic coding abilities, improved frontend code quality and practicality, and better context understanding
- kimi-k2-thinking: Context length 256k. A thinking model with general agentic and reasoning capabilities, specializing in deep reasoning tasks Usage Notes
- kimi-k2-turbo-preview: Context length 256k. High-speed version of kimi-k2, always aligned with the latest kimi-k2 (kimi-k2-0905-preview). Same model parameters as kimi-k2, output speed up to 60 tokens/sec (max 100 tokens/sec)
- kimi-k2-thinking-turbo: Context length 256k. High-speed version of kimi-k2-thinking, suitable for scenarios requiring both deep reasoning and extremely fast responses

- seed-1-6-250915: Maximum context length:256k Maximum input length:224 Maximum chain-of-thought content length:32k Can Set the maximum answer length：32k Default maximum response length:4k
- seed-1-6-flash-250715: Maximum context length: 256k Maximum input length: 224k Maximum chain of thought content length: 32k Can Set the maximum answer length：32k Default maximum response length: 4k
- deepseek-v3-1-250821: Maximum context length:128k Maximum input length:96k Maximum Chain of Thought Content Length：32k Can Set the maximum answer length：32k Default Maximum Response Length：4k