[
  {
    "model": "webApp.criterion",
    "pk": 1,
    "fields": {
      "name": "Code Repository",
      "key": "code",
      "description": "Provide the link at the repository, in the `url` field. If it's private also provide information on how to access it. If no code is provided put null. ONLY extract the text from CODE REPOSITORY and put it in the `extracted` field, give a score based on that text, explain the score in the `score_explanation` field. A good repository documentation contains clear, complete, and actionable information that allows an external reader to fully understand, set up, and reproduce the codebase without prior knowledge. It should describe the purpose of the project, the structure of the repository, installation instructions, dependency and environment requirements, configuration details, data sources and preprocessing steps, execution commands for every major component, and explicit instructions for reproducing results, including how each script maps to the outputs reported in the associated work. It must also explain key assumptions, and provide examples or tutorials that demonstrate correct usage. Ultimately, good documentation removes ambiguity, exposes all operational steps, and enables a third party to rerun the entire pipeline reliably.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 2,
    "fields": {
      "name": "Datasets",
      "key": "datasets",
      "description": "Provide ONLY the list of datasets NAMES that were used in this paper for training or evaluation. Be sure the datasets mentioned are used in the research because as some mentioned datasets may not actually be used in the paper (e.g., they may be referenced because they were not used, or as potential future additions).",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 3,
    "fields": {
      "name": "Annotation",
      "key": "annotation",
      "description": "A good annotation description tells about how data annotations or labels were created, validated, and quality-controlled. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about them but used them as a reference. The parts COULD be included are: how labels were produced, who performed the annotation, what tools or platforms were used, what annotation guidelines were followed, the expertise or training of annotators, reported inter-annotator agreement metrics (e.g., Cohen's kappa, Fleiss' kappa, Krippendorff's alpha, percent agreement), how disagreements were resolved (consensus, arbitration, majority vote), and any procedures used to check or enforce annotation reliability.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 4,
    "fields": {
      "name": "Preprocessing",
      "key": "preprocessing",
      "description": "A good prepocessing description tell about all transformations applied to raw data before training or evaluation. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about this criteria but used them as a reference. The parts COULD be included are:  explicit preprocessing steps such as resampling, cropping, normalization, and augmentation; numerical parameters such as input dimensions, value ranges, and augmentation probabilities; description of the augmentation pipeline; motivations for each preprocessing step; and any software libraries, frameworks, or pipelines used.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 5,
    "fields": {
      "name": "Evaluation",
      "key": "evaluation",
      "description": "A good evaluation description tell about completeness and transparency of the experimental evaluation setup. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about this criteria but used them as a reference. The parts COULD be included are: train/validation/test data splits, stratification, subject-wise splits, cross-validation strategies (e.g., k-fold, leave-one-out), performance metrics used, uncertainty or variability reporting (standard deviations, confidence intervals, repeated runs, random seeds), details of the experimental setup such as hyperparameters, epochs, hardware, search methods, and any robustness, ablation, or sensitivity analyses.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 6,
    "fields": {
      "name": "Licensing",
      "key": "licensing",
      "description": "A good Licensing description tells about legal, ethical, and compliance information for datasets, code, and experiments involving human subjects. If in the paper it's used code or data provided from somewhere else they should say which license there is on that regarding how another user can used that information. If they used a public dataset no license it's needed. You can find these information mainly in the code repository. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about them but used them as a reference. The parts COULD be included are:  dataset and code licensing terms (e.g., CC-BY, CC-BY-NC, MIT, GPL), any usage restrictions, ethical approvals or IRB statements, informed consent procedures, anonymization or pseudonymization methods, and discussions of privacy protection, risks, societal impact, bias, or limitations. Look for these information mainly in the CODE REPOSITORY provided",
      "weight": 1.0
    }
  }
]
