[
  {
    "model": "webApp.criterion",
    "pk": 1,
    "fields": {
      "name": "Code Repository",
      "key": "code",
      "description": "Provide the link at the repository, in the `url` field. If it's private also provide information on how to access it. If no code is provided put null. ONLY extract the text from CODE REPOSITORY and put it in the `extracted` field, give a score based on that text, explain the score in the `score_explanation` field. A good repository documentation contains clear, complete, and actionable information that allows an external reader to fully understand, set up, and reproduce the codebase without prior knowledge. It should describe the purpose of the project, the structure of the repository, installation instructions, dependency and environment requirements, configuration details, data sources and preprocessing steps, execution commands for every major component, and explicit instructions for reproducing results, including how each script maps to the outputs reported in the associated work. It must also explain key assumptions, and provide examples or tutorials that demonstrate correct usage. Ultimately, good documentation removes ambiguity, exposes all operational steps, and enables a third party to rerun the entire pipeline reliably.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 2,
    "fields": {
      "name": "Datasets",
      "key": "datasets",
      "description": "A good Datasets and Split description tells about the origin, characteristics, and the specific strategy used to partition data for training and testing. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about this criteria but used them as a reference. The parts COULD be included are: explicit availability of the data (citations, URLs, or version numbers); summary statistics (total number of samples, class balance/distribution, input resolution or format); precise definition of data splits (exact numbers or percentages for training, validation, and test sets); the splitting methodology (random, stratified, temporal, or subject-wise/patient-wise split to ensure independence); and measures taken to prevent data leakage between training and testing sets.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 3,
    "fields": {
      "name": "Annotation",
      "key": "annotation",
      "description": "A good annotation description explains how data annotations or labels were created, validated, and quality-controlled with respect to the datasets used in the paper. The following parts are only examples and are not mandatory; do not be too strict when assigning the score based on them, but use them as a reference. The aspects that could be included are: how labels were produced, who performed the annotation, which tools or platforms were used, which annotation guidelines were followed, the expertise or training of the annotators, reported inter-annotator agreement metrics (e.g., Cohen’s kappa, Fleiss’ kappa, Krippendorff’s alpha, percent agreement), how disagreements were resolved (consensus, arbitration, majority vote), and any procedures used to check or enforce annotation reliability. This criterion is NOT mandatory. If the focus of the paper is not the proposal of a dataset, there is no reason to consider annotations in the evaluation.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 4,
    "fields": {
      "name": "Preprocessing",
      "key": "preprocessing",
      "description": "A good prepocessing description tell about all transformations applied to raw data before training or evaluation. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about this criteria but used them as a reference. The parts COULD be included are:  explicit preprocessing steps such as resampling, cropping, normalization, and augmentation; numerical parameters such as input dimensions, value ranges, and augmentation probabilities; description of the augmentation pipeline; motivations for each preprocessing step; and any software libraries, frameworks, or pipelines used.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 5,
    "fields": {
      "name": "Evaluation",
      "key": "evaluation",
      "description": "A good evaluation description tell about completeness and transparency of the experimental evaluation setup. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about this criteria but used them as a reference. The parts COULD be included are: train/validation/test data splits, stratification, subject-wise splits, cross-validation strategies (e.g., k-fold, leave-one-out), performance metrics used, uncertainty or variability reporting (standard deviations, confidence intervals, repeated runs, random seeds), details of the experimental setup such as hyperparameters, epochs, hardware, search methods, and any robustness, ablation, or sensitivity analyses.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 6,
    "fields": {
      "name": "Licensing",
      "key": "licensing",
      "description": "A good Licensing description tells about legal, ethical, and compliance information for datasets, code, and experiments involving human subjects. If in the paper it's used code or data provided from somewhere else they should say which license there is on that regarding how another user can used that information. If they used a public dataset no license it's needed. You can find these information mainly in the code repository. The subsequent parts are only an example and not mandatory, don't be too strict when give the score about them but used them as a reference. The parts COULD be included are:  dataset and code licensing terms (e.g., CC-BY, CC-BY-NC, MIT, GPL), any usage restrictions, ethical approvals or IRB statements, informed consent procedures, anonymization or pseudonymization methods, and discussions of privacy protection, risks, societal impact, bias, or limitations. Look for these information mainly in the CODE REPOSITORY provided",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 7,
    "fields": {
      "name": "Datasets_2",
      "key": "datasets_2",
      "description": "A good Models and Algorithms description contains:\n* Formal Definition: A clear description of the mathematical setting (formulas, notations), the algorithm (pseudo-code), and the model architecture.\n* Analysis of Assumptions: An explicit explanation of any theoretical assumptions or simplifications adopted during development.\n* Environment Specifications: A declaration of the software framework used (e.g., PyTorch, TensorFlow, JAX) and the specific versions to ensure technical compatibility.",
      "weight": 1.0
    }
  },
  {
    "model": "webApp.criterion",
    "pk": 8,
    "fields": {
      "name": "Experiments_2",
      "key": "experiments_2",
      "description": "A good Experiments description contains:\n* Hyperparameter details: The range of hyperparameters considered, the method used to select the best configuration, and the final specification of all hyperparameters used to generate the reported results.\n* Robustness and Sensitivity: Information regarding performance sensitivity to parameter changes and the exact number of training and evaluation runs conducted.\n* Benchmarking: Clear details on how baseline methods were implemented, tuned, and compared fairly against the proposed method.\n* Evaluation Framework: A clear definition of the specific evaluation metrics, statistical tools, and precise details of the training/validation/testing splits used.\n* Results Analysis: A description of results including central tendency (e.g., mean), variation (e.g., error bars), and an analysis of the statistical significance of performance differences.\n* Resource Efficiency: The average runtime per result, estimated energy costs, memory footprint, and a description of the computing infrastructure (hardware and software) used.\n* Qualitative and Clinical Analysis: An analysis of failure cases (situations where the method did not perform well) and, where applicable, a discussion of the clinical significance of the findings.",
      "weight": 1.0
    }
  }
]
