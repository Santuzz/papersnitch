<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd" xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">µ 2 Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siyou</forename><surname>Li</surname></persName>
							<email>siyou.li@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengyao</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering and Physical Sciences</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huanan</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Nie</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Meta Inc. US</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arun</forename><forename type="middle">J</forename><surname>Thirunavukarasu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Nuffield Department of Clinical Neurosciences</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
							<email>juntao.yu@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
							<email>l.zhang.16@bham.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Engineering</orgName>
								<orgName type="department" key="dep2">College of Engineering and Physical Sciences</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">William Harvey Research Institute</orgName>
								<orgName type="institution" key="instit2">NIHR Barts Biomedical Research Centre</orgName>
								<orgName type="institution" key="instit3">Queen Mary University London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">µ 2 Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D874DE295341F8995E017768F63AC9F4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-12-22T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Radiology Report Generation</term>
					<term>Computed Tomography</term>
					<term>Tokenizer</term>
					<term>Multimodal Large Language Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expertwritten reports. To address these challenges, we propose µ 2 LLM, a multiscale multimodal large language models for RRG tasks. The novel µ 2 Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned µ 2 LLMs on limited data for RRG tasks. All code, data, and models will be publicly available in our official repository: <ref type="url" target="https://github.com/Siyou-Li/u2Tokenizer">https://github.com/Siyou-Li/u2Tokenizer</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Radiology reports are the primary means by which radiologists communicate their findings, likely diagnoses, and management recommendations to referring physicians and surgeons <ref type="bibr" target="#b20">[21]</ref>. These reports must be accurate and interpretable, as ambiguous language or mistakes can lead to clinical error as well as increased patient anxiety <ref type="bibr" target="#b14">[15]</ref>. Expert reports are especially important for imaging that referrers are frequently unable to interpret independently, such as computed tomography (CT). An increasing volume of CT examinations year-on-year generates pressure for radiologists to produce more high-quality reports, compounded by workforce shortages <ref type="bibr" target="#b3">[4]</ref>. Emerging artificial intelligence (AI) and natural language processing (NLP) technologies built upon foundation model architectures show promise in automating radiology report generation (RRG) <ref type="bibr" target="#b16">[17]</ref>. If accurate, automated RRG may streamline radiologist workflows, reduce reporting time, and enhance report quality. Automated RRG may also facilitate large-scale data extraction for clinical research, improving the usability of radiological data <ref type="bibr" target="#b11">[12]</ref>. Integrating AI into clinical practice may thereby enhance diagnostic accuracy, improve patient outcomes, and help meet the demand for healthcare services.</p><p>Existing RRG models are typically built around LLaVA <ref type="bibr" target="#b10">[11]</ref>, where input data consists of CT images resized to fixed dimensions. However, CT images exhibit variability in length, width, and height, and the resizing processes can distort anatomical details and lesions, potentially compromising diagnostic accuracy. Moreover, the direct incorporation of high-resolution CT images into analytical pipelines is inefficient and frequently prohibited by limited computational resources, rendering the comprehensive and efficient extraction of pertinent imaging data a critical challenge in the report generation. Additionally, there is no unified standard for structuring radiology reports; clinicians prioritize the content of the findings over strict character-level alignment. Traditional NLP evaluation metrics, such as BLEU <ref type="bibr" target="#b12">[13]</ref> and ROUGE <ref type="bibr" target="#b9">[10]</ref>, are therefore not wellsuited for evaluating RRG because they focus on lexical similarity rather than clinical salience or meaning. By comparing mainstream RRG models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b0">[1]</ref>, we identify two significant challenges: (1) limitations in image encoders, as conventional approaches that crop or downsample CT scans lead to significant information loss, particularly along organ boundaries; and (2) the inadequacy of conventional NLP evaluation methods, which fail to accurately capture the semantic and clinical relevance of generated reports compared to ground truth. Our contributions: we propose a novel automated RRG approach based on a multi-modal large language model (MLLM) named µ 2 LLM, as shown in Fig. <ref type="figure">1</ref>. Our framework is designed to efficiently and cost-effectively preserve critical imaging details through guided question integration within the MLLM. Central to our approach is the proposed µ 2 Tokenizer, an intermediate processing layer that applies multi-level attention mechanisms and multi-scale aggregation on the outputs of the visual tokenizer (ViT3D). This layer further incorporates multi-modal attention to seamlessly fuse input question embeddings with refined CT image embeddings, thereby maximizing their semantic correspondence while maintaining computational efficiency. For evaluation, we identify and elucidate clinically significant errors using the GREEN model <ref type="bibr" target="#b11">[12]</ref>-a specialized RRG metric that leverages large language model-based natural language understanding. To enhance the quality of generated reports, we employ direct preference optimization (DPO) <ref type="bibr" target="#b13">[14]</ref> to align model outputs with expert-validated clinical accuracy. Comprehensive evaluations conducted on three extensive CT imagereport datasets demonstrate that our method produces radiology reports that contain clinically salient points and are computationally efficient, addressing critical limitations exhibited by existing automated RRG models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Set-up</head><p>Given a CT image I and the corresponding question text Q, the RRG task aims to generate a report Ŷ that describes diagnostic findings Y by answering the given question Q.</p><p>The perceiver approaches <ref type="bibr" target="#b7">[8]</ref> do not consider the Q when compressing the image embeddings, which leads to suboptimal solutions. In our work, we introduce an intermediate tokenization layer, µ 2 Tokenizer, to effectively bridge the vision and language models. Figure <ref type="figure">1</ref> shows the pipeline of our model.</p><p>Image Encoder and Text Tokenizer: We scale the CT image I with height H, width W , and depth D, and then divide it into T frames each consisting of K slices. i.e. I ∈ R T ×K×H×W , T = D K . we employ a Vision Transformer (ViT3D) <ref type="bibr" target="#b0">[1]</ref> as our image encoder, which first transforms and splits each frame I i , i ∈ [0, T ) into frames and patches. The purpose of splitting the CT into several frames is to reduce the huge amount of computation caused by one-time input, and to avoid the information loss caused by direct downsampling and splitting. The image encoder then extracts local features for each patch and a global representation of the CT image. Formally, this produces a sequence of visual tokens: V ∈ R T ×Nv×E , where N v is the number of visual tokens, and E is the embedding dimension. Simultaneously, we obtain text tokens Q ∈ R Nq×E after tokenizing the question with the text tokenizer, where N q is the max length of the question. The µ 2 Tokenizer fuses the text tokens Q with visual tokens V to create compact visual tokens V ′ ∈ R Nv×E using a multi-scale multi-modal attention mechanism. This ensures that relevant image information is efficiently passed to the LLM while reducing computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale-specific learnable queries Visual tokens</head><p>Relative Positional Encoding Image encoder (ViT3D)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tokenizer + embed_token</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-temporal significance Scoring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differentiable Token Selection</head><p>Dynamic Multi-scale Token Pooling(DMTP)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear aggregation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-query interaction</head><p>Visual-query interaction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale visual tokens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text tokens</head><p>Fig. <ref type="figure">2</ref>: The illustration of our proposed µ 2 Tokenizer. The improvement is applied to steps of Token Selection, Multi-scale Pooling, and the Positional Encoding.</p><p>Report Generation: The processed image embeddings are then integrated with a text question to generate a radiology report. We utilize M3D-LaMed-Phi-3-4B <ref type="bibr" target="#b0">[1]</ref> as the base LLM for report decoding. The LLM takes as input both the textual question Q and the µ 2 Tokenizer-processed visual tokens V ′ , generating the radiology report Ŷ accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">µ 2 Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer</head><p>To improve the extraction efficiency for the CT image information, we propose the µ 2 Tokenizer module (as shown in Figure <ref type="figure">2</ref>), which can process CT images with an arbitrary number of slices and leverage the pre-trained model for efficient alignment training. This module is built upon the Linear Video Tokenizer (LinVT) <ref type="bibr" target="#b4">[5]</ref> that was originally introduced for the video understanding task. LinVT comprises two sub-modules: Spatio-temporal Visual Token Refiner (SVR) and Text-conditioned Token Aggregation (TTA). These modules adhere to the linearity principle, meaning that the output of each module is a linear combination of part of its input, thereby preserving the visual-language alignment learned in the image-LLM.</p><p>Relative Positional Encoding(RPE). The LinVT <ref type="bibr" target="#b4">[5]</ref> uses absolute learnable positional embeddings added along the frame and token dimensions. For the j th visual token V i,j in i th frame, two absolute positional embeddings (P f (i) and P t (j)) are added to the V i,j . Such an approach is not effective in capturing local relationships that are particularly useful in 3D volumes where local patterns matter. Instead, we use relative positional encoding so that the model can better capture local relationships regardless of the absolute position. The relative positional encoding is integrated within the attention mechanism <ref type="bibr" target="#b15">[16]</ref>. When computing the attention score between token i and token j, we add learned positional embeddings based on their relative position:</p><formula xml:id="formula_0">A ij = QiK ⊤ j √ d +P r (i-j).</formula><p>When used with multi-head attention, each head has its unique positional embeddings. Differentiable Token Selection(DTS). The LinVT <ref type="bibr" target="#b4">[5]</ref> uses a hard top-k selection which results in information loss when visual tokens are not selected. From a training perspective, a small k also leads to slow optimization as the error can not be effectively backpropagated back to non-selected visual tokens. To solve this limitation, we replace the hard selection with a fully differentiable soft selection mechanism. For each of the k selections, it computes a weight for all tokens and uses the weighted sum to produce a "soft" top token. This not only mitigates the information loss but also makes the selection process fully differentiable and improves gradient flow. The top-k soft tokens were calculated globally, taking into account visual tokens in all frames. Formally, for each of the k soft tokens we first compute an attention score α</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal</head><formula xml:id="formula_1">(r) = Sof tmax(W (r) s V f lat ) where W (r) s ∈ R E×1 , V f lat ∈ R T •Nv×E , r = 1, . . . , k. The soft token V (r)</formula><p>top is calculated as the weighted sum of all visual tokens:</p><formula xml:id="formula_2">V (r) top = T •Nv i=1 α (r) i V f lat (i).</formula><p>Dynamic Multi-scale Pooling (DMTP). The LinVT uses fixed pooling kernel sizes that treat individual kernels equally. We improved it to dynamic pooling, which allows the network to learn how to weight and choose the appropriate pooling. This is a more effective alternative to fixed pooling. We adapt the dynamic multi-scale pooling to weight the multiple pooled outputs dynamically. To implement this, we first apply an average pooling on different kernel sizes s ∈ [1, 2, 4]: y s = AvgPool(V top , kernel = s) and then compute dynamic weights w s via a small MLP g(•) over the mean of the pooled outputs:</p><formula xml:id="formula_3">w s = exp(g(y s ))</formula><p>s′∈S exp(g(y s′ )) . The weight w s is multiplied by y s to create weighted pooling outputs. Then, the final pooled representation is created by concatenating these weighted outputs. This allows the network to adapt the pooling operation based on the input distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Direct Preference Optimization with GREEN-Score</head><p>Our training consists of two stages. First, we perform supervised fine-tuning (SFT) using the CT-Reports dataset. Second, we adopted the Direct Preference Optimization(DPO) <ref type="bibr" target="#b13">[14]</ref> method (as shown in Figure <ref type="figure" target="#fig_1">3</ref>). In particular, we optimize our model on GREEN Score <ref type="bibr" target="#b11">[12]</ref>, which is arguably the most effective method to evaluate medical reports. GREEN <ref type="bibr" target="#b11">[12]</ref> effectively identifies significant discrepancies between the reference and generated reports, providing a detailed score from 0 to 1 for quantitative analysis and a summary for qualitative analysis. This interpretable evaluation helps improve the quality of automated radiology reporting. To obtain the preference dataset, we use the trained SFT model to generate a large number of medical reports on the existing dataset, and then these medical reports are scored by GREEN against the ground truth. Finally, the scored reports are used in DPO <ref type="bibr" target="#b13">[14]</ref> training, to guide the model generating preferred reports that have the highest GREEN score. As a result, the reports generated by our model are more accurate and semantically closer to human experts. Formally, the model is trained on the following DPO training objective:</p><formula xml:id="formula_4">L DP O (π θ ; π ref ) = -E (V,x,yw,y l )∼D DP O [log σ(β log π θ (y w |x) π ref (y w |x) -β π θ (y l |x) π ref (y l |x) )]</formula><p>with π θ is the policy model, π ref is reference model, σ is sigmoid function, β ∈ (0.1, 0.5), y w represent good responses and y l represent bad responses.</p><p>3 Experiments Datasets. AMOS-MM 2024 [9] consists of 2,088 chest, abdomen, and pelvis medical images and corresponding manually annotated text reports. The medical images are CT scans with spatial resolution from 256×256 to 1024×1024 and slice thickness from 1mm to 5mm. CT-Rate [7] consists of 50,188 CT images of 21,340 patients and corresponding text reports. The scanning resolution and slice numbers range from 256×256 to 1024×1024 and 46 to 1,277, respectively. AbdomenAltas 3.0 [3] is created by using RadGPT to generate reports for 17 public datasets, and through annotation, review, and refinement by 12 radiologists to ensure the reports' accuracy. It comprises over 1.8 million text tokens and 2.7 million images from 9,262 CT scans.</p><p>Furthermore, we expanded the dataset based on manually annotated medical reports using GPT-4o mini. This expansion included report rewriting and the generation of clinically relevant question-answer pairs, enriching the dataset's diversity and comprehensiveness for improved model training and evaluation.</p><p>Implementation Details. We preprocess the 3D CT images using Min-Max Normalization, then resizing and cropping to a standard dimension of 8 × 32 × 256 × 256, and a random noise added. Our 3D vision encoder employs a 3D ViT from M3D-CLIP <ref type="bibr" target="#b0">[1]</ref>, and base-LLM is Llama-3.2-1B-Instruct <ref type="bibr" target="#b17">[18]</ref>. All models are trained by AdamW optimizer with warm-up and cosine decay and use the bf16 mixed-precision training strategy enabled by DeepSpeed. Training is conducted in parallel across 4 NVIDIA A40 GPUs (48 GB VRAM each). For the µ 2 Tokenizer layer, we use four Spatio Temporal Attention Layers and four Text Condition Token Attention layers each consisting of eight attention heads and set k = 1024 for top soft token selection. For scale-specific learnable queries we use 1024 queries with a hidden size of 768. Baselines and Evaluation Metrics. We compare our model with several efficient and high-performing MLLMs, including LaMed-Phi-3-4B <ref type="bibr" target="#b0">[1]</ref>, LaMed-Llama-2-7B <ref type="bibr" target="#b0">[1]</ref>, CT-CHAT(Llama-3.1-8B) <ref type="bibr" target="#b5">[6]</ref>, RadGPT-N <ref type="bibr" target="#b2">[3]</ref>, and RadFM-14B <ref type="bibr" target="#b18">[19]</ref>, which excel at capturing linguistic patterns and generating coherent text across various domains. We also include the comparison of our model µ 2 LLM-1B (SFT) with only SFT, and our model µ 2 LLM-1B (SFT&amp;DPO) with both SFT and DPO. Given the complexity of evaluating content accuracy between generated reports and human references, we employ both traditional and LLM-based metrics. Traditional metrics include BLEU <ref type="bibr" target="#b12">[13]</ref>, ROUGE <ref type="bibr" target="#b9">[10]</ref>, METEOR <ref type="bibr" target="#b1">[2]</ref>, and BERT-Score <ref type="bibr" target="#b19">[20]</ref>, which quantify text similarity through n-gram overlap and variations, although they have limited semantic understanding. LLM-based metrics, i.e. the GREEN score <ref type="bibr" target="#b11">[12]</ref>, utilize models with strong semantic comprehension to evaluate the alignment between generated reports and human references. This metric assesses matching content and errors, offering a more comprehensive report quality measure.</p><p>Results Analysis. Our model demonstrates superior performance compared to baseline models across multiple datasets. Notably, despite its significantly smaller scale (1B parameters, only 14% of comparable models ranging from 7B to 14B), our model consistently outperforms these larger counterparts. Table <ref type="table" target="#tab_1">1</ref> presents a comparative evaluation across different datasets. Our model achieves state-of-the-art results, outperforming existing approaches. For instance, on the CT-Rate dataset, our model attains ROUGE-1 = 0.517, METEOR = 0.330, BERTScore = 0.879, and GREEN = 0.384, significantly surpassing CT-CHAT- 8B. These results underscore the effectiveness of our approach, particularly after SFT and DPO. The integration of the GREEN Score-based dataset selection for DPO fine-tuning further enhances model performance, leading to more accurate and clinically relevant report generation. Specifically, the GREEN Score evaluation indicates a model capability improvement of 20% with DPO in GREEN Score, with the most substantial gain observed on the AMOS-MM dataset, where the GREEN Score increased from 0.33 to 0.40. To further assess the impact of individual components, we conducted ablation experiments (Table <ref type="table" target="#tab_2">2</ref>). These experiments confirm that each enhancement contributes meaningfully to model performance. When training parameters remain consistent, incorporating DTS yields the most substantial performance boost, improving the GREEN Score by up to 0.2 points. This finding highlights the effectiveness of DTS in optimizing token representation and selection, leading to more accurate and clinically meaningful text generation. Figure <ref type="figure">4</ref> illustrates an example report produced by µ 2 LLM-1B. On the left, a 3D heat map visualizes the Question-CT cross-attention scores, indicating the regions of the CT scan most relevant to the model's diagnostic reasoning. The center and right images depict the original CT scan and corresponding problem statement, where the model is tasked with identifying and diagnosing abnormalities in the abdominal region. The rightmost section displays the generated radiology report, which provides a structured interpretation of the CT findings. The report includes descriptions of liver parenchyma density, gallbladder mor-Report Can you provide a diagnosis based on the fingings in abdomen in this image?</p><p>The liver parenchyma demonstrates decreased density, with a non-dilated intrahepatic and extrahepatic bile ducts, while the gallbladder is not enlarged and shows no thickening of its wall, and both the size and shape of the spleen, pancreas, bilateral kidneys, and adrenal glands are normal, with the renal parenchyma also showing no abnormal density and no dilation of the renal pelvis and calyces, alongside normal morphology and morphology of the bilateral ureters and the absence of abnormal density; additionally, ... CT Image Fig. <ref type="figure">4</ref>: An example of generated report from our µ 2 LLM-1B (SFT&amp;DPO). phology, renal pelvis dilation, and other critical observations. The generated text demonstrates clinical coherence and diagnostic accuracy, aligning with standard radiology interpretations. This visualization highlights the model's capability to focus on diagnostically relevant regions and produce detailed, structured radiology reports, supporting its potential use in automated medical imaging analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we introduced µ 2 Tokenizer, a multi-scale, multi-modal middleware, and a DPO optimization framework for radiology report generation. By integrating ViT3D with an LLM, our approach effectively combines visual and textual information, enabling accurate and coherent medical reports. To refine the model for RRG tasks, we utilized the GREEN-Model and SFT to curate high-quality datasets for DPO fine-tuning, improving alignment with clinical standards. Despite limited training data, our model outperformed larger baselines, particularly in GREEN Score, demonstrating the effectiveness of multi-modal fusion and optimization techniques in automated radiology reporting. These results highlight the importance of structured fine-tuning in enhancing diagnostic accuracy. Moving forward, our approach could be further extended to other medical imaging modalities and clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 : 2</head><label>12</label><figDesc>Fig.1: Overview of our proposed µ 2 LLM model that is centered with the µ 2 Tokenizer layer for high quality RRG task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Overview of training process with Direct Preference Optimization (DPO).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance Comparison Across Different Datasets</figDesc><table><row><cell>Datasets</cell><cell>Models</cell><cell cols="4">ROUGE-1 METEOR BERTScore GREEN</cell></row><row><cell/><cell>LaMed-Phi-3-4B</cell><cell>0.136</cell><cell>0.058</cell><cell>0.807</cell><cell>0.011</cell></row><row><cell/><cell>LaMed-Llama-2-7B</cell><cell>0.139</cell><cell>0.060</cell><cell>0.810</cell><cell>0.009</cell></row><row><cell>AbdomenAltas</cell><cell>RadFM-14B</cell><cell>0.037</cell><cell>0.013</cell><cell>0.794</cell><cell>0.000</cell></row><row><cell/><cell>RadGPT-N</cell><cell>0.247</cell><cell>0.112</cell><cell>-</cell><cell>-</cell></row><row><cell/><cell>µ 2 LLM-1B(SFT)</cell><cell>0.529</cell><cell>0.295</cell><cell>0.891</cell><cell>0.281</cell></row><row><cell/><cell>µ 2 LLM-1B(SFT&amp;DPO)</cell><cell>0.567</cell><cell>0.319</cell><cell>0.895</cell><cell>0.346</cell></row><row><cell/><cell>LaMed-Phi-3-4B</cell><cell>0.130</cell><cell>0.050</cell><cell>0.814</cell><cell>0.002</cell></row><row><cell/><cell>LaMed-Llama-2-7B</cell><cell>0.103</cell><cell>0.048</cell><cell>0.815</cell><cell>0.001</cell></row><row><cell>CT-Rate</cell><cell>RadFM-14B</cell><cell>0.054</cell><cell>0.017</cell><cell>0.812</cell><cell>0.014</cell></row><row><cell/><cell>CT-CHAT-8B</cell><cell>0.294</cell><cell>0.221</cell><cell>0.815</cell><cell>0.113</cell></row><row><cell/><cell>µ 2 LLM-1B(SFT)</cell><cell>0.517</cell><cell>0.330</cell><cell>0.879</cell><cell>0.384</cell></row><row><cell/><cell>µ 2 LLM-1B(SFT&amp;DPO)</cell><cell>0.539</cell><cell>0.359</cell><cell>0.890</cell><cell>0.429</cell></row><row><cell/><cell>LaMed-Phi-3-4B</cell><cell>0.126</cell><cell>0.047</cell><cell>0.821</cell><cell>0.009</cell></row><row><cell/><cell>LaMed-Llama-2-7B</cell><cell>0.163</cell><cell>0.065</cell><cell>0.823</cell><cell>0.009</cell></row><row><cell>AMOS-MM</cell><cell>RadFM-14B</cell><cell>0.046</cell><cell>0.015</cell><cell>0.812</cell><cell>0.001</cell></row><row><cell/><cell>µ 2 LLM-1B(SFT)</cell><cell>0.421</cell><cell>0.249</cell><cell>0.881</cell><cell>0.339</cell></row><row><cell/><cell>µ 2 LLM-1B(SFT&amp;DPO)</cell><cell>0.459</cell><cell>0.876</cell><cell>0.881</cell><cell>0.400</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance Comparison component effectiveness of µ 2 Tokenizer</figDesc><table><row><cell>Model</cell><cell cols="5">BLEU ROUGE-1 METEOR BERTScore GREEN</cell></row><row><cell>Baseline</cell><cell>0.190</cell><cell>0.405</cell><cell>0.210</cell><cell>0.864</cell><cell>0.204</cell></row><row><cell>+RPE</cell><cell>0.281</cell><cell>0.421</cell><cell>0.236</cell><cell>0.880</cell><cell>0.277</cell></row><row><cell>+DTS</cell><cell>0.271</cell><cell>0.411</cell><cell>0.240</cell><cell>0.888</cell><cell>0.299</cell></row><row><cell>+DMTP</cell><cell>0.254</cell><cell>0.401</cell><cell>0.220</cell><cell>0.874</cell><cell>0.233</cell></row><row><cell>µ 2 LLM-1B(SFT)</cell><cell>0.279</cell><cell>0.421</cell><cell>0.249</cell><cell>0.881</cell><cell>0.339</cell></row><row><cell cols="2">µ 2 LLM-1B(SFT&amp;DPO) 0.336</cell><cell>0.459</cell><cell>0.876</cell><cell>0.881</cell><cell>0.400</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disclosure of Interests</head><p>The authors have no competing interests in the paper</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m">M3d: Advancing 3d medical image analysis with multi-modal large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</editor>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06">Jun 2005</date>
			<biblScope unit="page" from="65" to="72"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Bassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Decherchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.04678</idno>
		<title level="m">Radgpt: Constructing 3d image-text tumor datasets</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">Everlight: Radiology unlocked: The global radiologist report 2025</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.05185</idno>
		<title level="m">Linvt: Empower your imagelevel large language model to understand videos</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Developing generalist foundation models from a mul</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Hamamci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Almas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Esirgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Dasdelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Durugol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Amiranashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Erdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alanbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lafci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ozdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>timodal dataset for 3d computed tomography</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Hamamci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Almas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Simsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Esirgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Dasdelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simsar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.17834</idno>
		<title level="m">A foundation model utilizing chest ct volumes and radiology reports for supervised-level zero-shot detection of abnormalities</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4651" to="4664"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Amos: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhanng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36722" to="36732"/>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">Jul 2004</date>
			<biblScope unit="page" from="74" to="81"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GREEN: Generative radiology report evaluation and error notation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ostmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blankemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E M</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Delbrouck</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-emnlp.21</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.findings-emnlp.21"/>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2024</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024-11">Nov 2024</date>
			<biblScope unit="page" from="374" to="390"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135"/>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</editor>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07">Jul 2002</date>
			<biblScope unit="page" from="311" to="318"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Differences in perceptions among radiologists, referring physicians, and patients regarding language for incidental findings reporting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Rosenkrantz</surname></persName>
		</author>
		<idno type="DOI">10.2214/AJR.16.16633</idno>
		<ptr target="https://doi.org/10.2214/AJR.16.16633"/>
		<imprint>
			<publisher>American Roentgen Ray Society</publisher>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page" from="140" to="143"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large language models in medicine</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Thirunavukarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S J</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Elangovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-023-02448-8</idno>
		<idno>1038/s41591-023-02448-8</idno>
		<ptr target="https://doi.org/10"/>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1930" to="1940"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Towards generalist foundation model for radiology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno>CoRR abs/1904.09675</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">102714</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>